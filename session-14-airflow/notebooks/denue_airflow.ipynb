{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c74878-429a-4011-896f-b921e126216d",
   "metadata": {},
   "source": [
    "### Importing libraries, setting Logging level and Constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7a7dadda-40c1-4a4d-87cb-9623d88d51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Setting LOG level and format, for mor info visit https://www.logicmonitor.com/blog/python-logging-levels-explained\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Values to change according your own configs\n",
    "PROFILE_NAME =  \"dsa-gepp\"\n",
    "S3_BUCKET = 'my-s3-bucket-john'\n",
    "FILE_NAME = \"denue_inegi_09_.csv\"\n",
    "URL = 'https://www.inegi.org.mx/contenidos/masiva/denue/denue_09_csv.zip'\n",
    "\n",
    "# Constant values\n",
    "SECRET_NAME = \"airflow/connections/dev/rds-mysql\"\n",
    "REGION_NAME = \"us-east-1\"\n",
    "S3_KEY_ORIGINAL = f\"session-14-airflow/denue/original/{FILE_NAME}\"\n",
    "S3_KEY_FINAL = f\"session-14-airflow/denue/final/{FILE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ac54e",
   "metadata": {},
   "source": [
    "### Getting the AWS RDS MySQL credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6b1fde9a-b9c7-4aaf-a70e-9c6c92eebb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret(secret, region, profile=\"default\"):\n",
    "    \"\"\"\n",
    "    Get the secret from AWS Secrets Manager\n",
    "    \n",
    "    :param secret (str): Name of the Secret\n",
    "    :param region (str): Region in which the Secret is stored\n",
    "    :param profile (str): Profile name to be executed, left in blank to take default (optional)\n",
    "    \n",
    "    :return: String with the Secret Value\n",
    "    \"\"\"    \n",
    "    \n",
    "    secret_name = secret\n",
    "    region_name = region\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    # Decrypts secret using the associated KMS key.\n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "\n",
    "    return secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8bbbcc3d-13ec-49ea-96ba-ac7679b24e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'admin', 'password': 'dataGepp5', 'engine': 'mysql', 'host': 'data-course-wize.c2jptj5kvukd.us-east-1.rds.amazonaws.com', 'port': 3306, 'dbname': 'datawzcourse', 'dbInstanceIdentifier': 'data-course-wize'}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print(get_secret(SECRET_NAME, REGION_NAME, PROFILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356f851-abbb-4a1a-bf76-b5bfaf265281",
   "metadata": {},
   "source": [
    "### Method to upload a file to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "85df5ee1-55fa-4002-b181-1b12a89cf3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(bucket_name, s3_key, file_object, region, profile=\"default\"):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket\n",
    "    \n",
    "    :param file_path (str): Local file path of the file to upload\n",
    "    :param bucket_name (str): Name of the S3 bucket to upload the file to\n",
    "    :param s3_key (str): S3 key (i.e. object name) to assign to the file in the bucket\n",
    "    :param region (str): Region in which the Secret is stored\n",
    "    :param profile (str): Profile name to be executed, left in blank to take default (optional)\n",
    "    \n",
    "    :return: botocore.response object. If error, returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create S3 client\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    s3_client = session.client(\n",
    "        service_name='s3',\n",
    "        region_name=region\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Upload file to S3 bucket from local\n",
    "        # s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        \n",
    "        # Upload file to S3 bucket from memory (Body)\n",
    "        response = s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=file_object)\n",
    "        \n",
    "        logging.info(f\"Successfully uploaded {s3_key} to S3 bucket {bucket_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error uploading file to S3: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3948f9a-35f4-4c4e-b592-8bb62e5291bd",
   "metadata": {},
   "source": [
    "### Method to get a file to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "659141ca-6b10-4f84-a62b-ec98b3348c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_object(bucket_name, object_name, region, profile=\"default\"):\n",
    "    \"\"\"\n",
    "    Retrieve an object from an S3 bucket\n",
    "    \n",
    "    :param bucket_name: string\n",
    "    :param object_name: string\n",
    "    \n",
    "    :return: botocore.response object. If error, returns None.\n",
    "    \"\"\"\n",
    "    # Create S3 client\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    s3_client = session.client(\n",
    "        service_name='s3',\n",
    "        region_name=region\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_name)\n",
    "        logging.info(f\"Successfully get {object_name} from S3 bucket {bucket_name}\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        logging.info(f\"Error getting object {object_name} from bucket {bucket_name}: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42990951-432a-47dd-9c19-76c87072bbb3",
   "metadata": {},
   "source": [
    "### Extracting/Downloading the Data (Denue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "45038ef3-4b60-42e7-8b51-86325d295905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def download_and_upload_to_s3(bucket_name, s3_key, url):\n",
    "    \"\"\"\n",
    "    Download the data from the external URL and upload it \n",
    "    decompressed to s3\n",
    "    \n",
    "    :param bucket_name (str): Bucket name\n",
    "    :param s3_key (str): S3 key that includes the whole path with the file name\n",
    "    :param url (str): URL of the external data\n",
    "    \n",
    "    :return: botocore.response object. If error, returns None.\n",
    "    \"\"\"  \n",
    "    \n",
    "    try:\n",
    "        # Download file from URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Extract CSV file from ZIP archive\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_filename = [f for f in z.namelist() if f.endswith(s3_key.split('/')[-1])][0]\n",
    "            csv_file = z.read(csv_filename)\n",
    "\n",
    "        # Uploading the data to s3\n",
    "        response = upload_file_to_s3(bucket_name, s3_key, csv_file, REGION_NAME, PROFILE_NAME)\n",
    "\n",
    "        return response\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.info(f\"Failed to download data from {url}: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    except zipfile.BadZipFile as e:\n",
    "        logging.info(f\"Failed to extract CSV file from ZIP archive: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    except boto3.exceptions.Boto3Error as e:\n",
    "        logging.info(f\"Failed to upload {key_name} to S3 bucket {bucket_name}: {e}\")\n",
    "        raise e\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "718d3586-f44f-4460-858a-98d60f8006aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto_de_datos/denue_inegi_09_.csv\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(URL)\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    csv_filename = [f for f in z.namelist() if f.endswith(S3_KEY.split('/')[-1])][0]\n",
    "    print(csv_filename)\n",
    "    #csv_file = z.read(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "950bfd0a-9736-4454-94c6-b94d67af8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO : Successfully uploaded session-14-airflow/denue/original/denue_inegi_09_.csv to S3 bucket my-s3-bucket-john\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "download_and_upload_to_s3(S3_BUCKET, S3_KEY_ORIGINAL, URL)\n",
    "#print(URL)\n",
    "#https://www.inegi.org.mx/contenidos/masiva/denue/denue_09_csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04359e68-91de-4f92-85f5-04d0e2a4883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "session = boto3.session.Session(profile_name='dsa-gepp')\n",
    "s3_client = session.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Define the S3 bucket and file path\n",
    "bucket_name = 'my-s3-bucket-john'\n",
    "file_path = 'session-14-airflow/denue/original/denue_inegi_09_.csv'\n",
    "\n",
    "# Read the CSV file from S3 using Pandas\n",
    "s3_file = s3_client.get_object(Bucket=bucket_name, Key=file_path)\n",
    "df_denue = pd.read_csv(s3_file['Body'], encoding='latin-1')\n",
    "\n",
    "df_denue.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6b925-5097-4e28-bf3b-78bcb6f2ac46",
   "metadata": {},
   "source": [
    "### Downloading Locally the data (because there is an issue with the encode when we use the s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4bb35289-0208-44b5-a341-d868f02d2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_content(bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Gets the object.\n",
    "    \n",
    "    :param bucket_name (str): Bucket name\n",
    "    :param s3_key (str): S3 key that includes the whole path with the file name\n",
    "\n",
    "    :return: The object data in as a dataframe with correct column names\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        response = get_s3_object(bucket_name, s3_key, REGION_NAME, PROFILE_NAME)\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "        logging.info(status)\n",
    "        \n",
    "        # Writing the file to the local file system\n",
    "        with open(f\"/tmp/{FILE_NAME}\",'wb') as output_file:\n",
    "            output_file.write(response.get(\"Body\").read())\n",
    "        \n",
    "        logging.info(f'Download file /tmp/{FILE_NAME} completed')\n",
    "        \n",
    "        if status == 200:\n",
    "            # Get columns for df\n",
    "            df = pd.read_csv(f\"/tmp/{FILE_NAME}\", encoding='latin-1')\n",
    "            return df\n",
    "        else:\n",
    "            raise ClientError\n",
    "\n",
    "    except ClientError as e:\n",
    "        logging.info(\"Couldn't get object from bucket .\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error when creating dataframe\")\n",
    "        logging.info(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba349ca4-15e7-4584-bbb5-19afb9fec380",
   "metadata": {},
   "source": [
    "### Cleaning, transforming and uploading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "db502a77-6a92-483f-9a99-74a906dcbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_data, bucket_name, s3_key):\n",
    "    # Filter by activities\n",
    "    options = [461110,461111,461112,461113]\n",
    "    gepp_data = df_data[df_data[\"codigo_act\"].isin(options)]\n",
    "    \n",
    "    # cod_postal column should always have 5 digits\n",
    "    gepp_data = gepp_data.astype({'cod_postal': 'string'})\n",
    "    gepp_data[\"cod_postal\"] = gepp_data[\"cod_postal\"].str.replace(\".0\",'' , regex=False)\n",
    "    gepp_data.dropna(subset=['cod_postal'], inplace=True)\n",
    "    gepp_data[\"cod_postal\"] = gepp_data[\"cod_postal\"].apply(lambda x: x if len(x) == 5 else '0'+ x)\n",
    "    \n",
    "    # The entity is always Mexico City and the key is always 9\n",
    "    gepp_data[\"is_city_valid\"] = gepp_data.apply(lambda row : True if  row['cve_ent'] == 9 and row['entidad'] == \"CIUDAD DE MÃ‰XICO\" else False , axis=1)    \n",
    "    final_data = gepp_data[gepp_data[\"is_city_valid\"]]\n",
    "    final_data.drop(columns=\"is_city_valid\", inplace=True)\n",
    "    \n",
    "    # Uploading the data to s3\n",
    "    response = upload_file_to_s3(bucket_name, s3_key, final_data.to_csv(), REGION_NAME, PROFILE_NAME)\n",
    "    \n",
    "    if response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 200:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb785511-dccc-4325-8ea9-15985d98c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO : Successfully get session-14-airflow/denue/original/denue_inegi_09_.csv from S3 bucket my-s3-bucket-john\n",
      "INFO : 200\n",
      "INFO : Download file /tmp/denue_inegi_09_.csv completed\n",
      "/var/folders/xt/xyzrgq412p51pshkvhsj0b2m0000gp/T/ipykernel_25128/3001233026.py:24: DtypeWarning: Columns (35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"/tmp/{FILE_NAME}\", encoding='latin-1')\n",
      "INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO : Successfully uploaded session-14-airflow/denue/final/denue_inegi_09_.csv to S3 bucket my-s3-bucket-john\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "df_denue = get_body_content(S3_BUCKET, S3_KEY_ORIGINAL)\n",
    "#df_denue.head()\n",
    "\n",
    "clean_data(df_denue, S3_BUCKET, S3_KEY_FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356091b-6663-4228-be5d-14516fa32301",
   "metadata": {},
   "source": [
    "### Python code to upload the data from s3 to Mysql(RDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "854fdeb7-f139-48d6-bdd4-5b9467d7da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "def upload_csv_to_mysql(table_name):\n",
    "    # Getting the data curated\n",
    "    df = pd.read_csv(f\"/tmp/{FILE_NAME}\", encoding='latin-1')\n",
    "    \n",
    "    # Getting the MySQL credentials\n",
    "    mysql_credentials = get_secret(SECRET_NAME, REGION_NAME, PROFILE_NAME)\n",
    "    \n",
    "    # Connect to MySQL database\n",
    "    conn = pymysql.connect(\n",
    "        host=mysql_credentials['host'],\n",
    "        user=mysql_credentials['username'],\n",
    "        password=mysql_credentials['password'],\n",
    "        port=mysql_credentials['port'],\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn.select_db(mysql_credentials['dbname'])\n",
    "    \n",
    "    # Create table from dataframe schema\n",
    "    with conn.cursor() as cursor:\n",
    "        columns = ['`{}` VARCHAR(255) NOT NULL'.format(col) for col in df.columns]\n",
    "        query = 'CREATE TABLE IF NOT EXISTS `{}` ({}) ENGINE=InnoDB'.format(table_name, ','.join(columns))\n",
    "        cursor.execute(query)\n",
    "    \n",
    "    # Load data into MySQL table\n",
    "    with conn.cursor() as cursor:\n",
    "        # Truncate table if exists\n",
    "        cursor.execute(f\"TRUNCATE TABLE `{table_name}`\")\n",
    "        # Create insert statement\n",
    "        placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "        columns = ', '.join(['`{}`'.format(col) for col in df.columns])\n",
    "        query = 'INSERT INTO `{}` ({}) VALUES ({})'.format(table_name, columns, placeholders)\n",
    "        # Execute the insert statement for each row\n",
    "        for row in df.itertuples(index=False):\n",
    "            cursor.execute(query, row)\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1460e16-72b3-44f4-be87-cd380e42bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_csv_to_mysql('test_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23abc6f-3044-4285-b90e-0d04971f5b82",
   "metadata": {},
   "source": [
    "## Puting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620d7ae-cc39-42e9-b9ce-36948251cec6",
   "metadata": {},
   "source": [
    "### Extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3fcad229-e9e3-49af-be00-394fad460f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_denue_inegi(**kwargs):\n",
    "    \"\"\"\n",
    "    This function will extract the data from Inegi\n",
    "    then will be decompressed and uploaded in csv files\n",
    "    into a S3 bucket (Original)\n",
    "    \"\"\"\n",
    "\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "    s3_key = kwargs['s3_key']\n",
    "    url = kwargs['url']\n",
    "    \n",
    "    download_and_upload_to_s3(bucket_name, s3_key, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "278bdf87-04f6-47e9-bbfa-b1e8b5c21a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO : Successfully uploaded session-14-airflow/denue/original/denue_inegi_09_.csv to S3 bucket my-s3-bucket-john\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "op_kwargs={\n",
    "        'bucket_name': S3_BUCKET, \n",
    "        's3_key': S3_KEY_ORIGINAL, \n",
    "        'url': URL                \n",
    "    }\n",
    "\n",
    "extract_denue_inegi(**op_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcfbd7-503c-4bb5-92ed-5d3cdf46296d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transform Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57aef88-3179-462b-ae12-e2699b0afa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_denue_inegi(**kwargs):\n",
    "    \"\"\"\n",
    "    This function will transform the data from Inegi\n",
    "    will take the data stored in S3, then download it \n",
    "    locally and perform some transformations and at the\n",
    "    end the data will be stored in S3 (Final)\n",
    "    \"\"\"\n",
    "\n",
    "    bucket_name = kwargs['bucket_name']\n",
    "    s3_key_original = kwargs['s3_key_original']\n",
    "    s3_key_final = kwargs['s3_key_final']\n",
    "    \n",
    "    df_denue = get_body_content(bucket_name, s3_key_original)\n",
    "    clean_data(df_denue, bucket_name, s3_key_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bac5ab-af70-4e71-a5f5-a5a586382cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "op_kwargs={\n",
    "        'bucket_name': S3_BUCKET, \n",
    "        's3_key_original': S3_KEY_ORIGINAL, \n",
    "        's3_key_final': S3_KEY_FINAL                \n",
    "    }\n",
    "\n",
    "transform_denue_inegi(**op_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
